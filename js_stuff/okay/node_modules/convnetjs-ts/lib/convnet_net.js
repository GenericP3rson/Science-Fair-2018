Object.defineProperty(exports, "__esModule", { value: true });
var util = require("./convnet_util");
var convnet_layers_loss_1 = require("./convnet_layers_loss");
var convnet_layers_dotproducts_1 = require("./convnet_layers_dotproducts");
var convnet_layers_nonlinearities_1 = require("./convnet_layers_nonlinearities");
var convnet_layers_pool_1 = require("./convnet_layers_pool");
var convnet_layers_input_1 = require("./convnet_layers_input");
var convnet_layers_dropout_1 = require("./convnet_layers_dropout");
var convnet_layers_normalization_1 = require("./convnet_layers_normalization");
var assert = util.assert;
/**
 * Net manages a set of layers
 * For now constraints: Simple linear order of layers, first layer input last layer a cost layer
 */
var Net = (function () {
    function Net(options) {
        if (!options) {
            options = [];
        }
        this.layers = [];
    }
    // takes a list of layer definitions and creates the network layer objects
    Net.prototype.makeLayers = function (defs) {
        // few checks
        assert(defs.length >= 2, 'Error! At least one input layer and one loss layer are required.');
        assert(defs[0].type === 'input', 'Error! First layer must be the input layer, to declare size of inputs');
        // desugar layer_defs for adding activation, dropout layers etc
        var desugar = function (defs) {
            var new_defs = new Array();
            for (var i = 0; i < defs.length; i++) {
                var def = defs[i];
                if (def.type === 'softmax' || def.type === 'svm') {
                    var lossDef = def;
                    // add an fc layer here, there is no reason the user should
                    // have to worry about this and we almost always want to
                    new_defs.push({ type: 'fc', num_neurons: lossDef.num_classes });
                }
                if (def.type === 'regression') {
                    // add an fc layer here, there is no reason the user should
                    // have to worry about this and we almost always want to
                    new_defs.push({ type: 'fc', num_neurons: def.num_neurons });
                }
                if (def.type === 'fc' || def.type === 'conv') {
                    var dotDef = def;
                    if (typeof (dotDef.bias_pref) === 'undefined') {
                        dotDef.bias_pref = 0.0;
                        if (typeof dotDef.activation !== 'undefined' && dotDef.activation === 'relu') {
                            dotDef.bias_pref = 0.1; // relus like a bit of positive bias to get gradients early
                            // otherwise it's technically possible that a relu unit will never turn on (by chance)
                            // and will never get any gradient and never contribute any computation. Dead relu.
                        }
                    }
                }
                new_defs.push(def);
                if (typeof def.activation !== 'undefined') {
                    if (def.activation === 'relu') {
                        new_defs.push({ type: 'relu' });
                    }
                    else if (def.activation === 'sigmoid') {
                        new_defs.push({ type: 'sigmoid' });
                    }
                    else if (def.activation === 'tanh') {
                        new_defs.push({ type: 'tanh' });
                    }
                    else if (def.activation === 'maxout') {
                        // create maxout activation, and pass along group size, if provided
                        var gs = def.group_size !== 'undefined' ? def.group_size : 2;
                        new_defs.push({ type: 'maxout', group_size: gs });
                    }
                    else {
                        console.log('ERROR unsupported activation ' + def.activation);
                    }
                }
                if (typeof def.drop_prob !== 'undefined' && def.type !== 'dropout') {
                    new_defs.push({ type: 'dropout', drop_prob: def.drop_prob });
                }
            }
            return new_defs;
        };
        defs = desugar(defs);
        // create the layers
        this.layers = [];
        for (var i = 0; i < defs.length; i++) {
            var def = defs[i];
            if (i > 0) {
                var prev = this.layers[i - 1];
                def.in_sx = prev.out_sx;
                def.in_sy = prev.out_sy;
                def.in_depth = prev.out_depth;
            }
            switch (def.type) {
                case 'fc':
                    this.layers.push(new convnet_layers_dotproducts_1.FullyConnLayer(def));
                    break;
                case 'lrn':
                    this.layers.push(new convnet_layers_normalization_1.LocalResponseNormalizationLayer(def));
                    break;
                case 'dropout':
                    this.layers.push(new convnet_layers_dropout_1.DropoutLayer(def));
                    break;
                case 'input':
                    this.layers.push(new convnet_layers_input_1.InputLayer(def));
                    break;
                case 'softmax':
                    this.layers.push(new convnet_layers_loss_1.SoftmaxLayer(def));
                    break;
                case 'regression':
                    this.layers.push(new convnet_layers_loss_1.RegressionLayer(def));
                    break;
                case 'conv':
                    this.layers.push(new convnet_layers_dotproducts_1.ConvLayer(def));
                    break;
                case 'pool':
                    this.layers.push(new convnet_layers_pool_1.PoolLayer(def));
                    break;
                case 'relu':
                    this.layers.push(new convnet_layers_nonlinearities_1.ReluLayer(def));
                    break;
                case 'sigmoid':
                    this.layers.push(new convnet_layers_nonlinearities_1.SigmoidLayer(def));
                    break;
                case 'tanh':
                    this.layers.push(new convnet_layers_nonlinearities_1.TanhLayer(def));
                    break;
                case 'maxout':
                    this.layers.push(new convnet_layers_nonlinearities_1.MaxoutLayer(def));
                    break;
                case 'svm':
                    this.layers.push(new convnet_layers_loss_1.SVMLayer(def));
                    break;
                default: console.log('ERROR: UNRECOGNIZED LAYER TYPE: ' + def.type);
            }
        }
    };
    // forward prop the network.
    // The trainer class passes is_training = true, but when this function is
    // called from outside (not from the trainer), it defaults to prediction mode
    Net.prototype.forward = function (V, is_training) {
        if (typeof (is_training) === 'undefined') {
            is_training = false;
        }
        var act = this.layers[0].forward(V, is_training);
        for (var i = 1; i < this.layers.length; i++) {
            act = this.layers[i].forward(act, is_training);
        }
        return act;
    };
    Net.prototype.getCostLoss = function (V, y) {
        this.forward(V, false);
        var N = this.layers.length;
        var loss = this.layers[N - 1].backward(y);
        return loss;
    };
    /**
     * backprop: compute gradients wrt all parameters
     */
    Net.prototype.backward = function (y) {
        var N = this.layers.length;
        var loss = this.layers[N - 1].backward(y); // last layer assumed to be loss layer
        for (var i = N - 2; i >= 0; i--) {
            this.layers[i].backward();
        }
        return loss;
    };
    Net.prototype.getParamsAndGrads = function () {
        // accumulate parameters and gradients for the entire network
        var response = [];
        for (var i = 0; i < this.layers.length; i++) {
            var layer_reponse = this.layers[i].getParamsAndGrads();
            for (var j = 0; j < layer_reponse.length; j++) {
                response.push(layer_reponse[j]);
            }
        }
        return response;
    };
    Net.prototype.getPrediction = function () {
        // this is a convenience function for returning the argmax
        // prediction, assuming the last layer of the net is a softmax
        var S = this.layers[this.layers.length - 1];
        assert(S.layer_type === 'softmax', 'getPrediction function assumes softmax as last layer of the net!');
        if (S instanceof convnet_layers_loss_1.SoftmaxLayer) {
            var p = S.out_act.w;
            var maxv = p[0];
            var maxi = 0;
            for (var i = 1; i < p.length; i++) {
                if (p[i] > maxv) {
                    maxv = p[i];
                    maxi = i;
                }
            }
            return maxi; // return index of the class with highest class probability
        }
        throw Error("to getPrediction, the last layer must be softmax");
    };
    Net.prototype.toJSON = function () {
        var json = {};
        json.layers = [];
        for (var i = 0; i < this.layers.length; i++) {
            json.layers.push(this.layers[i].toJSON());
        }
        return json;
    };
    Net.prototype.fromJSON = function (json) {
        this.layers = [];
        for (var i = 0; i < json.layers.length; i++) {
            var Lj = json.layers[i];
            var t = Lj.layer_type;
            var L = void 0;
            if (t === 'input') {
                L = new convnet_layers_input_1.InputLayer();
            }
            if (t === 'relu') {
                L = new convnet_layers_nonlinearities_1.ReluLayer();
            }
            if (t === 'sigmoid') {
                L = new convnet_layers_nonlinearities_1.SigmoidLayer();
            }
            if (t === 'tanh') {
                L = new convnet_layers_nonlinearities_1.TanhLayer();
            }
            if (t === 'dropout') {
                L = new convnet_layers_dropout_1.DropoutLayer();
            }
            if (t === 'conv') {
                L = new convnet_layers_dotproducts_1.ConvLayer();
            }
            if (t === 'pool') {
                L = new convnet_layers_pool_1.PoolLayer();
            }
            if (t === 'lrn') {
                L = new convnet_layers_normalization_1.LocalResponseNormalizationLayer();
            }
            if (t === 'softmax') {
                L = new convnet_layers_loss_1.SoftmaxLayer();
            }
            if (t === 'regression') {
                L = new convnet_layers_loss_1.RegressionLayer();
            }
            if (t === 'fc') {
                L = new convnet_layers_dotproducts_1.FullyConnLayer();
            }
            if (t === 'maxout') {
                L = new convnet_layers_nonlinearities_1.MaxoutLayer();
            }
            if (t === 'svm') {
                L = new convnet_layers_loss_1.SVMLayer();
            }
            L.fromJSON(Lj);
            this.layers.push(L);
        }
    };
    return Net;
}());
exports.Net = Net;
//# sourceMappingURL=convnet_net.js.map