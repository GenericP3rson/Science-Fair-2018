import { Net } from "./convnet_net";
import { LayerOptions } from "./layers";
import { cnnutil } from "./index";
import { TrainerOptions, Trainer } from "./convnet_trainers";
/**
 * An agent is in state0 and does action0
 * environment then assigns reward0 and provides new state, state1
 * Experience nodes store all this information, which is used in the
 * Q-learning update step
 */
export declare class Experience {
    state0: number[];
    action0: number;
    reward0: number;
    state1: number[];
    constructor(state0?: number[], action0?: number, reward0?: number, state1?: number[]);
}
export interface BrainOptions {
    temporal_window?: number;
    experience_size?: number;
    start_learn_threshold?: number;
    learning_steps_total?: number;
    learning_steps_burnin?: number;
    epsilon_min?: number;
    epsilon_test_time?: number;
    random_action_distribution?: number[];
    gamma?: number;
    layer_defs?: LayerOptions[];
    hidden_layer_sizes?: number[];
    tdtrainer_options?: TrainerOptions;
}
export interface PolycyResult {
    action: number;
    value: number;
}
/**
 * A Brain object does all the magic.
 * over time it receives some inputs and some rewards
 * and its job is to set the outputs to maximize the expected reward
 */
export declare class Brain {
    temporal_window: number;
    experience_size: number;
    start_learn_threshold: number;
    learning_steps_total: number;
    learning_steps_burnin: number;
    epsilon_min: number;
    epsilon_test_time: number;
    random_action_distribution: number[];
    gamma: number;
    net_inputs: number;
    num_states: number;
    num_actions: number;
    window_size: number;
    state_window: number[][];
    action_window: number[];
    reward_window: number[];
    net_window: number[][];
    value_net: Net;
    tdtrainer: Trainer;
    /**
     * experience replay
     */
    experience: Experience[];
    /** incremented every backward() */
    age: number;
    /** incremented every forward()*/
    forward_passes: number;
    /** controls exploration exploitation tradeoff. Should be annealed over time */
    epsilon: number;
    latest_reward: number | number[];
    last_input_array: number[];
    average_reward_window: cnnutil.Window;
    average_loss_window: cnnutil.Window;
    learning: boolean;
    constructor(num_states: number, num_actions: number, opt?: BrainOptions);
    random_action(): number;
    policy(s: number[] | Float64Array): PolycyResult;
    getNetInput(xt: number[]): number[];
    forward(input_array: number[]): number;
    backward(reward: number): void;
    visSelf(elt: HTMLElement): void;
}
