Object.defineProperty(exports, "__esModule", { value: true });
var tslib_1 = require("tslib");
var layers_1 = require("./layers");
var util = require("./convnet_util");
/**
 * An inefficient dropout layer
 * Note this is not most efficient implementation since the layer before
 * computed all these activations and now we're just going to drop them :(
 * same goes for backward pass. Also, if we wanted to be efficient at test time
 * we could equivalently be clever and upscale during train and copy pointers during test
 * todo: make more efficient.
 */
var DropoutLayer = (function (_super) {
    tslib_1.__extends(DropoutLayer, _super);
    function DropoutLayer(opt) {
        var _this = this;
        if (!opt) {
            return;
        }
        var dopt = opt;
        _this = _super.call(this, dopt) || this;
        // computed
        _this.out_sx = dopt.in_sx;
        _this.out_sy = dopt.in_sy;
        _this.out_depth = dopt.in_depth;
        _this.layer_type = 'dropout';
        _this.drop_prob = typeof dopt.drop_prob !== 'undefined' ? dopt.drop_prob : 0.5;
        var d = util.zeros(_this.out_sx * _this.out_sy * _this.out_depth);
        _this.dropped = d.map(function (v) { return v !== 0; });
        return _this;
    }
    DropoutLayer.prototype.forward = function (V, is_training) {
        this.in_act = V;
        if (typeof (is_training) === 'undefined') {
            is_training = false;
        } // default is prediction mode
        var V2 = V.clone();
        var N = V.w.length;
        if (is_training) {
            // do dropout
            for (var i = 0; i < N; i++) {
                if (Math.random() < this.drop_prob) {
                    V2.w[i] = 0;
                    this.dropped[i] = true;
                } // drop!
                else {
                    this.dropped[i] = false;
                }
            }
        }
        else {
            // scale the activations during prediction
            for (var i = 0; i < N; i++) {
                V2.w[i] *= this.drop_prob;
            }
        }
        this.out_act = V2;
        return this.out_act; // dummy identity function for now
    };
    DropoutLayer.prototype.backward = function () {
        var V = this.in_act; // we need to set dw of this
        var chain_grad = this.out_act;
        var n = V.w.length;
        V.dw = util.zeros(n); // zero out gradient wrt data
        for (var i = 0; i < n; i++) {
            if (!(this.dropped[i])) {
                V.dw[i] = chain_grad.dw[i]; // copy over the gradient
            }
        }
    };
    DropoutLayer.prototype.getParamsAndGrads = function () {
        return [];
    };
    DropoutLayer.prototype.toJSON = function () {
        var json = {};
        json.out_depth = this.out_depth;
        json.out_sx = this.out_sx;
        json.out_sy = this.out_sy;
        json.layer_type = this.layer_type;
        json.drop_prob = this.drop_prob;
        return json;
    };
    DropoutLayer.prototype.fromJSON = function (json) {
        this.out_depth = json.out_depth;
        this.out_sx = json.out_sx;
        this.out_sy = json.out_sy;
        this.layer_type = json.layer_type;
        this.drop_prob = json.drop_prob;
        var d = util.zeros(this.out_sx * this.out_sy * this.out_depth);
        this.dropped = d.map(function (v) { return v !== 0; });
    };
    return DropoutLayer;
}(layers_1.LayerBase));
exports.DropoutLayer = DropoutLayer;
//# sourceMappingURL=convnet_layers_dropout.js.map